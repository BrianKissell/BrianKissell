ggplot(word_counts, aes(x = word2, y = n, fill = factor(Cond))) +
# Don't include the legend for the column plot
geom_col(show.legend=FALSE) +
# Facet by candidate and make the y-axis free
facet_wrap(~condition, scales = "free_y") +
# Flip the coordinates and add a title: "Twitter Word Counts"
coord_flip() +
scale_fill_manual(values=c("red", "blue", "pink"))+
labs(title = "Frequently Used Words based on Experimental Group", x = "Words Used", y = "Frequency")
ggplot(word_counts, aes(x = word2, y = n, fill = condition)) +
# Don't include the legend for the column plot
geom_col(show.legend=FALSE) +
# Facet by candidate and make the y-axis free
facet_wrap(~condition, scales = "free_y") +
# Flip the coordinates and add a title: "Twitter Word Counts"
coord_flip() +
scale_fill_manual(values=c("red", "blue", "pink"))+
labs(title = "Frequently Used Words based on Experimental Group", x = "Words Used", y = "Frequency")
df_text <- read.delim("textual_response_data.txt")
df_text <- df_text %>%
mutate(condition = factor(Cond, levels = c(1,2,3), labels = c("Control", "Cold", "Overvalue")))
glimpse(df_text)
#Create a wordcloud that shows the unique words from the three groups. Also, do a simple count related to the use of personal pronouns.
#Load packages
library(tidyverse)
library(tidytext)
library(wordcloud)
library(tidytext)
library(tm)
library(topicmodels)
library(kableExtra)
# Create a custom dictionary for stop words.
custom_stop_words <- tribble(
# Column names need to match match stop_words
~word, ~lexicon,
# Add http, win, t.co, etc., as custom stop words
"http", "CUSTOM",
"https", "CUSTOM",
"rt", "CUSTOM",
"t.co", "CUSTOM",
"â", "CUSTOM",
"iâ", "CUSTOM",
"itâ", "CUSTOM",
"ve", "CUSTOM",
"ll", "CUSTOM",
"19", "CUSTOM",
"tâ", "CUSTOM",
"aâ", "CUSTOM",
"theâ", "CUSTOM",
"canâ", "CUSTOM",
"de", "CUSTOM",
"amp", "CUSTOM",
"ðÿ", "CUSTOM",
"weâ", "CUSTOM"
)
# Bind the custom stop words to stop_words
stop_words2 <- stop_words %>%
bind_rows(custom_stop_words)
# Tokenize the textual data from the Tweets.
tidy_tweets <- df_text %>%
unnest_tokens(word, text) %>%
#Remove stop words
anti_join(stop_words2)
# Obtain the top ten most frequently used words from each candidate (after removing stop words).
word_counts <- tidy_tweets %>%
count(word, condition) %>%
group_by(condition) %>%
top_n(20, n) %>%
ungroup() %>%
mutate(word2 = fct_reorder(word, n))
# Include a color aesthetic tied to the candidate's name.
ggplot(word_counts, aes(x = word2, y = n, fill = condition)) +
# Don't include the legend for the column plot
geom_col(show.legend=FALSE) +
# Facet by candidate and make the y-axis free
facet_wrap(~condition, scales = "free_y") +
# Flip the coordinates and add a title: "Twitter Word Counts"
coord_flip() +
scale_fill_manual(values=c("red", "blue", "pink"))+
labs(title = "Frequently Used Words based on Experimental Group", x = "Words Used", y = "Frequency")
install.packages("qdap")
df_text <- read.delim("textual_response_data.txt")
df_text <- df_text %>%
mutate(condition = factor(Cond, levels = c(1,2,3), labels = c("Control", "Cold", "Overvalue")))
glimpse(df_text)
# Load qdap
library(qdap)
# Find the 10 most frequent terms: term_count
term_count <- freq_terms(df_text$text, 10)
# Plot term_count
plot(term_count)
#### Try something else
# Load qdap
library(qdap)
install.packages("qdap")
library(tm)
# Create a DataframeSource from the example text
df_source <- DataframeSource(df_text)
str(df_text)
DataframeSource(df_text)
is.na(df_text$text)
text_df <- df_text %>%
select(doc_id = Subject, condition, text)
library(tidyverse)
text_df <- df_text %>%
select(doc_id = Subject, condition, text)
# Create a DataframeSource from the example text
df_source <- DataframeSource(text_df)
# Convert df_source to a volatile corpus
df_corpus <- VCorpus(df_source)
# Examine df_corpus
df_corpus
# Examine df_corpus metadata
meta(df_corpus)
glimpse(bk01)
bk01_to_join <- bk01 %>%
select(doc_id = ï..id, sex, age, npi_total)
text_df <- text_df %>%
inner_join(bk01_to_join, by=doc_id)
text_df <- df_text %>%
select(doc_id = Subject, condition, text)
bk01_to_join <- bk01 %>%
select(doc_id = ï..id, sex, age, npi_total)
text_df <- text_df %>%
inner_join(bk01_to_join, by=doc_id)
bk01_to_join
text_df <- text_df %>%
inner_join(bk01_to_join, by="doc_id")
# Create a DataframeSource from the example text
df_source <- DataframeSource(text_df)
# Convert df_source to a volatile corpus
df_corpus <- VCorpus(df_source)
# Examine df_corpus
df_corpus
# Examine df_corpus metadata
meta(df_corpus)
#### Try something else
# Load qdap
library(qdap)
install.packages("devtools")
install_github("qdapDictionaries", "trinker")
library(devtools)
install_github("qdapDictionaries", "trinker")
install_github("qdap", "trinker")
install.packages("Rtools")
install.Rtools(choose_version = TRUE, check = FALSE, GUI = TRUE,
page_with_download_url = "https://cran.r-project.org/bin/windows/Rtools/",
...)
install.packages("installr")
install.Rtools()
library(installr)
install.Rtools()
install_github("qdap", "trinker")
library(Rtools)
install.Rtools()
install.Rtools(choose_version = TRUE, check = FALSE, GUI = TRUE,
page_with_download_url = "https://cran.r-project.org/bin/windows/Rtools/")
knitr::opts_chunk$set(echo = TRUE)
anova_1 <- aov(npi_total ~ condition, data = bk01)
glimpse(anova_1)
library(broom)
tidy(anova_1)
tidy(anova_1)[1,.]
tidy(anova_1)[[1,.]]
glimpse(tidy(anova_1))
glimpse(tidy(anova_1)%>%filter(term == "condition"))
anova_1 <- aov(npi_total ~ condition, data = bk01) %>%
tidy() %>%
filter(term == "condition")
anova_1$statistic
round(anova_1$statistic, 2)
round(anova_1$statistic, 3)
round(anova_1$statistic, 1)
round(anova_1$statistic, 4)
round(anova_1$statistic, 2)
round(anova_1$statistic, 3)
?round(
round(anova_1$statistic, 3)
?round(anova_1$statistic, 3)
signif(anova_1$statistic, 3)
signif(anova_1$statistic, 4)
ceiling(anova_1$statistic, 4)
round(anova_1$statistic, 2)
anova_1 <- aov(npi_total ~ condition, data = bk01) %>%
tidy()
round(anova_1$statistic, 2)
anova_1
anova_1[.,.]
anova_1[1,]
round(anova_1[1,]$statistic, 2)
round(anova_1[1,]$statistic, 2)
anova_1
anova_1[1,]$df
anova_1[2,]$df
anova_1[1,]$df
round(anova_1[1,]$p.value, 3)
?broom
broom::augment(aov(npi_total ~ condition, data = bk01))
broom::glance(aov(npi_total ~ condition, data = bk01))
broom::tidy(aov(npi_total ~ condition, data = bk01))
?tidy()
?broom::tidy
anova_1
anova_1[1,]$sumsq/(anova_1[1,]$sumsq+anova_1[2,]$sumsq)
round(anova_1[1,]$sumsq/(anova_1[1,]$sumsq+anova_1[2,]$sumsq), 3)
?emmeans()
install.packages("emmeans")
?emmeans()
?emmeans
library(emmeans)
?emmeans()
emmeans(aov(npi_total ~ condition, data = bk01))
emmeans(aov(npi_total ~ condition, data = bk01), ~condition)
glimpse(anova_1_means)
anova_1_means <- emmeans(aov(npi_total ~ condition, data = bk01), ~condition)
glimpse(anova_1_means)
anova_1_means <- emmeans(aov(npi_total ~ condition, data = bk01), ~condition) %>% tidy()
glimpse(anova_1_means)
anova_1_means[1,]$estimate
round(anova_1_means[1,]$estimate, 2)
if(!"tinytex" %in% rownames(installed.packages())) install.packages("tinytex")
tinytex::install_tinytex()
anova_2
#Conduct analysis of variance with sex added.
anova_2 <- aov(npi_total ~ condition * sex, data = bk01) %>% tidy()
anova_2_means <- emmeans(aov(npi_total ~ condition * sex, data = bk01), ~condition | sex) %>% tidy()
anova_2
anova_2
# Install devtools package if necessary
if(!"devtools" %in% rownames(installed.packages())) install.packages("devtools")
# Install the stable development verions from GitHub
devtools::install_github("crsh/papaja")
anova_2_means
anova_2_means
glimpse(anova_2_means)
summary(emmeans(aov(npi_total ~ condition * gender, data = bk01), ~condition | gender))
# Load packages
library(tidyverse)
library(ggthemes)
theme_set(theme_gdocs())
#Import data
setwd("C:/Users/Brian/DataScience")
df <- read.csv("bk01.csv")
# Clean and prepare data
bk01 <- df %>%
mutate(
condition = factor(condition, levels = c(1,2,3), labels = c("Control", "Cold", "Overvalue")),
gender = factor(sex, levels = c(0,1), labels = c("Male", "Female")),
npi_total = npi_total / 40
)
#Conduct analysis of variance
library(broom)
library(emmeans)
anova_1 <- aov(npi_total ~ condition, data = bk01) %>% tidy()
anova_1_means <- emmeans(aov(npi_total ~ condition, data = bk01), ~condition) %>% tidy()
#Conduct analysis of variance with gender added.
anova_2 <- aov(npi_total ~ condition * gender, data = bk01) %>% tidy()
anova_2_means <- emmeans(aov(npi_total ~ condition * gender, data = bk01), ~condition | gender) %>% tidy()
summary(emmeans(aov(npi_total ~ condition * gender, data = bk01), ~condition | gender))
?emmeans
glimpse(emmeans(aov(npi_total ~ condition * gender, data = bk01), ~condition | gender))
emmeans(aov(npi_total ~ condition * gender, data = bk01), ~condition | gender).emmc
emmeans(aov(npi_total ~ condition * gender, data = bk01), npi_total ~ condition * gender)
emmeans(aov(npi_total ~ condition * gender, data = bk01), ~ condition * gender)
emmeans(aov(npi_total ~ condition * gender, data = bk01), ~ condition | gender)
anova_2_means_gender <- emmeans(aov(npi_total ~ condition * gender, data = bk01), ~ gender)
anova_2_means_gender
emmeans(aov(npi_total ~ condition * gender, data = bk01), ~ gender) %>% tidy()
anova_2_means_gender <- emmeans(aov(npi_total ~ condition * gender, data = bk01), ~ gender) %>% tidy()
anova_2_means_gender
anova_2_means
knitr::opts_chunk$set(echo = TRUE)
# Load packages
library(tidyverse)
install.packages("tidyverse")
install.packages("backports")
library(tidyverse)
library(tidyverse)
library(tidyverse)
library(qdap)
library(tidyverse)
library(askpass)
detach("package:askpass", unload = TRUE)
library(clipr)
detach("package:clipr", unload = TRUE)
library(tidyverse)
remove.packages("tidyverse", lib="~/R/win-library/4.0")
library(qdap)
library(qdapTools)
remove.packages("qdap", lib="~/R/win-library/4.0")
library(blogdown)
detach("package:blogdown", unload = TRUE)
1
1+2+
3
library(qdapTools)
remove.packages("qdapDictionaries", lib="~/R/win-library/4.0")
remove.packages("qdapRegex", lib="~/R/win-library/4.0")
remove.packages("qdapTools", lib="~/R/win-library/4.0")
install.packages("tidyverse")
library(tidyverse)
library(tidyverse)
install.packages("dplyr")
library(dyplr)
library(dpylr)
library(dplyr)
.libPaths()
.libPaths(c("C:/Program Files/R/R-4.0.2/library", .libPaths())
)
.libPaths()
library(tidyverse)
.libpaths()[1,]
.libPaths()[1,]
.libPaths()[1]
.libPaths() <- .libPaths()[1]
.libPaths()
?install.packages()
library(tidyverse)
Sys.getenv("PATH")
Sys.which("stats.dll")
Sys.getenv("PATH")
Sys.getenv('R_USER')
Modify environment variables
Sys.getenv("PATH") <- Sys.getenv('R_USER')
Sys.setenv("C:/Users/Brian/Documents")
Sys.getenv("PATH")
Sys.which("stats.dll")
Sys.setenv("C:\\Users\\Brian\\Documents")
Sys.setenv("R_USER")
.libPaths()
myPaths <- .libPaths()   # get the paths
myPaths <- c(myPaths[2], myPaths[1])  # switch them
.libPaths(myPaths)  # reassign them
library("tidyverse")
library(tidyverse)
.libPaths()
install.packages("tidyverse")
library(tidyverse)
install.packages("tidyverse")
library(tidyverse)
.libPaths()
Sys.getenv("PATH")
Sys.which("stats.dll")
PATH
Sys.getenv("C:/Users/Brian/Documents/R/win-library/4.0")
Sys.getenv("PATH")
?Sys.getenv
Sys.setenv(PATH = "C:/Users/Brian/Documents/R/win-library/4.0")
Sys.getenv("PATH")
library(tidyverse)
Sys.setenv(PATH = "C:\Program Files\R\R-4.0.2\bin;")
Sys.setenv(PATH = "C:\Program Files\R\R-4.0.2\bin")
Sys.setenv(PATH = "C://Program Files//R//R-4.0.2//bin;")
Sys.getenv("PATH")
library(tidyverse)
Sys.getenv("PATH")
Sys.setenv(PATH = "C://Program Files//R//R-4.0.2;")
library(tidyverse)
.libPaths()
myPaths <- .libPaths()   # get the paths
myPaths <- c(myPaths[2], myPaths[1])
.libPaths(myPaths)
.libPaths()
library(tidyverse)
install.packages("dplyr")
library(dplyr)
install.packages("dplyr")
library(dplyr)
library(dplyr)
library(dplyr)
Sys.getenv()
Sys.setenv(PATH = "C://Users\\Brian;")
library(dplyr)
data.frame(seq(1,100,2), seq(1,50,1))
data.frame(col1 = seq(1,100,2), col2 = seq(1,50,1))
data.frame(col1 = seq(1,100,2), col2 = seq(1,50,1)) %>%
mutate(col3 = ifelse(col1 >= median(col1),col1+100, col1-100))
load("E:/Projects/general_conference_project/data_scraping/scraped_data/gc_all.Rdata")
nrows(gc_all)
nrow(gc_all)
glimpse(gc_all)
library(tidyverse)
library(dplyr)
glimpse(gc_all)
min(gc_all$talk_year)
table(gc_all$talk_year)
install.packages("stringr")
library(stringr)
str_word_count(gc_all$text)
x<- "hello"
str_count(x)
str_length(x)
# Split speeches into words
words <- str_split(gc_all$text, pattern = " ")
# Number of words per line
words_per_talk <- lapply(words, length)
head(words_per_talk)
sum(words_per_talk)
count(words_per_talk)
summarize(sum(words_per_talk))
words_per_talk
lapply(words_per_talk,sum)
sum(words_per_talk)
mean(words_per_talk)
mean(as.numeric(words_per_talk))
sum(as.numeric(words_per_talk))
install.packages("quanteda.dictionaries")
library(quanteda.dictionaries)
install.packages("quanteda")
x <- sample(.72,2,100)
x
x <- sample(.72, 100)
x <- sample(1:2, 100)
x <- rbinom(100)
x <- rbinom(100,2,.72)
x
x <- rbinom(100,1,.72)
x
x <- rbinom(100,1,.72)
y <- rbinom(100,1,.72)
z <- ifelse(x != y, 1, 0)
z
nrows(z[[1]])
nrows(z[1])
library(dplyr)
nrows(filter(z == 1))
filter(z == 1)
nrows(z == 1)
z == 1
z[z == 1]
nrows(z[z == 1])
nrow(z[z == 1])
sum(z[z == 1])
x1 <- rbinom(sum(z[z == 1]),1,.28)
y1 <- rbinom(sum(z[z == 1]),1,.72)
z1 <- ifelse(x != y, 1, 0)
z1
sum(z[z == 1]
sum(z[z == 1])
sum(z1[z1 == 1])
y1
z1 <- ifelse(x != y, 1, 0)
z1
z1 <- ifelse(x1 != y1, 1, 0)
sum(z1[z1 == 1])
x <- rbinom(1000,1,.28)
y <- rbinom(1000,1,.72)
z <- ifelse(x != y, 1, 0)
sum(z[z == 1])
x <- rbinom(10000,1,.28)
y <- rbinom(10000,1,.72)
z <- ifelse(x != y, 1, 0)
sum(z[z == 1])
x <- rbinom(100000,1,.28)
y <- rbinom(100000,1,.72)
z <- ifelse(x != y, 1, 0)
sum(z[z == 1])
x <- rbinom(1000000,1,.28)
y <- rbinom(1000000,1,.72)
z <- ifelse(x != y, 1, 0)
sum(z[z == 1])
x1 <- rbinom(sum(z[z == 1]),1,.28)
y1 <- rbinom(sum(z[z == 1]),1,.72)
z1 <- ifelse(x1 != y1, 1, 0)
sum(z1[z1 == 1])
sum(z1[z1 == 1])/sum(z1)
sum(z1[z1 == 1])/sum(z)
.5^8
.5^7
.5^6
.5^5
pbinom(50,8,.49)
rbinom(50,8,.49)
rbinom(50000,8,.49)
kids[kids == 8]
kids<-rbinom(50000,8,.49)
kids[kids == 8]
sum(kids[kids == 8])/50000
kids<-rbinom(500000,8,.51)
sum(kids[kids == 8])/50000
sum(kids[kids == 8])/500000
.02515+.037248
(.49^8)^50 + (.51^8)^50
1-(.49^8)^50 + (.51^8)^50
1-(.49^8 + .51^8)^50
(.49^8 + .51^8)^50
.49^8
.51^8
.49^8 + .51^8
(.49^8 + .51^8)^50
kids<-rbinom(500000,8,.49)
sum(kids[kids == 8])/500000
1-sum(kids[kids == 8])/500000
kids<-rbinom(500000,8,.49)
1-sum(kids[kids == 8])/500000
kids<-rbinom(500000,8,.51)
1-sum(kids[kids == 8])/500000
kids<-rbinom(500000,8,.49)
1-sum(kids[kids == 8|kids == 0])/500000
sum(kids[kids == 8|kids == 0])/500000
.51^8
.49^4
pbinom(1, 8, .49)
pbinom(0, 8, .49)
pbinom(1, 8, .49)
pbinom(1, 8, .49) + pbinom(1, 8, .51)
(pbinom(1, 8, .49) + pbinom(1, 8, .51))^50
1-(pbinom(1, 8, .49) + pbinom(1, 8, .51))^50
quanteda()
library(quanteda)
install.packages("quanteda.dictionaries")
setwd("C:/Users/Brian/DataScience/BrianKissell")
